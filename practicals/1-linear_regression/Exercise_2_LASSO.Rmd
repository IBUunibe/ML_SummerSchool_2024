---
title: "Exercise 2: LASSO analysis"
subtitle: ""
author:
  - name: "Aparna Pandey, Marco Kreuzer & Stephan Peischl"
    affiliation: "IBU Machine Learning Summer School 2024, University of Bern"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output:
  html_document:
    self_contained: true
    toc: true # table of content true
    toc_float: true
    toc_depth: 2  # upto two depths of headings (specified by #, ## and ###)
    theme: spacelab
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(GGally)
library(glmnet)
library(ggplot2)
library(gridExtra)
```


We consider the pyrimidine data set presented in the lecture. It contains 74 activity measurements of the enzyme DHFR in a bacterium in the presence of different pyrimidines characterized by 26 physico-chemical properties. Those properties are quantified by the variables X1 to X26; the activity of DHFR is the response variable Y.

Reference: Jonathan D Hirst, Ross D King, and Michael JE Sternberg. Quantitative structure-activity relationships by neural
networks and inductive logic programming. i. the inhibition of dihydrofolate reductase by pyrimidines. Journal of
Computer-Aided Molecular Design, 8(4):405–420, 1994.

## Exercise Questions:

1. What is the optimal range for the regularization parameter lambda?
  - [−5;−3]
  - [0.0001; 0.001]
  - [0.005; 0.05]
  - [0.1; 1]
  - [3; 5]

2. When choosing lambda = 0.018 (left dotted line in left plot), how many non-zero coefficients do we get? Write down the model equation.

3. Assume now that we choose lambda = 0.05 (right dotted line in left plot). Which variables remain in the model in this case?

## Data Exploration

First, lets load the pyrimidine data set:

```{r}
pyrimidine <- read.csv("data/pyrimidine.csv")
```

 Let's print the last few variables and the response:
 
```{r}
# Load the data set
n <- nrow(pyrimidine)
p <- ncol(pyrimidine) - 1

# Preview the dataset
pyrimidine[1:3,22:27]
```
```{r}
summary(pyrimidine)
```

Do we have any missing values?

```{r}
# Check for missing values
missing_values <- colSums(is.na(pyrimidine))
missing_values
```

```{r, fig.height=12, fig.width=8}
numeric_columns <- sapply(pyrimidine, is.numeric)
numeric_data <- pyrimidine[, numeric_columns]

# Create histograms for numeric columns
plots <- list()
for (col in names(numeric_data)) {
  g <- ggplot(pyrimidine, aes_string(x = col)) +
    geom_histogram(binwidth = 0.05, fill = "blue", color = "black") +
    labs(title = paste("Histogram of", col), x = col, y = "Frequency")
  plots[[col]] <- g
}

# Arrange and plot histograms
do.call("grid.arrange", c(plots, ncol = 3))
```


## Lasso Regression Analysis


```{r}
# Set the seed for reproducibility
set.seed(307)

# Prepare the data for glmnet
X <- as.matrix(pyrimidine[, -(p+1)])
y <- pyrimidine$y

# Perform Lasso regression
pyr.lasso <- glmnet(X, y)

# Plot the Lasso model
plot(pyr.lasso, xvar = "lambda")

```


```{r}
# Cross-validation to find the optimal lambda
pyr.lasso.cv <- cv.glmnet(X, y)

# Plot the cross-validation results
plot(pyr.lasso.cv)

# Display the optimal lambda values
lambda_min <- pyr.lasso.cv$lambda.min
lambda_1se <- pyr.lasso.cv$lambda.1se

lambda_min
lambda_1se

```

```{r}
# Coefficients at the optimal lambda (lambda.1se)
coef_1se <- coef(pyr.lasso.cv, s = lambda_1se)
print("Coefficients at lambda.1se:")
print(coef_1se)

# Predictions at lambda.min for the first observation
prediction <- predict(pyr.lasso.cv, newx = as.matrix(pyrimidine[1, -ncol(pyrimidine)]), s = lambda_min)
print("Prediction for the first observation at lambda.min:")
prediction

# Actual value for the first observation
actual_value <- pyrimidine$y[1]
print("Actual value for the first observation:")
actual_value

```

Question A
What is the optimal range for the regularization parameter lambda?

- [−5;−3]
- [0.0001; 0.001]
- [0.005; 0.05]
- [0.1; 1]
- [3; 5]


Question B

When choosing λ = 0.018 (left dotted line in left plot), how many non-zero coefficients do we get? Write down the model equation.

```{r}
# Coefficients at lambda = 0.018
coef_018 <- coef(pyr.lasso.cv, s = 0.018)
print("Coefficients at lambda = 0.018:")
print(coef_018)

# Count non-zero coefficients
num_non_zero <- sum(coef_018 != 0)
print("Number of non-zero coefficients at lambda = 0.018:")
num_non_zero

# Display model equation (only non-zero coefficients)
print("Model equation at lambda = 0.018:")
coef_018[coef_018 != 0]

```

Question C
Assume now that we choose λ = 0.05 (right dotted line in left plot). Which variables remain in the model in this case?

```{r}
# Coefficients at lambda = 0.05
coef_050 <- coef(pyr.lasso.cv, s = 0.05)
print("Coefficients at lambda = 0.05:")
print(coef_050)

# Display non-zero coefficients
print("Variables remaining in the model at lambda = 0.05:")
coef_050[coef_050 != 0]

```

