---
title: "Exercise 1: Regression analysis (cement dataset)"
subtitle: "Data Modelling and Multicolinearity"
author:
  - name: "Aparna Pandey, Marco Kreuzer & Stephan Peischl"
    affiliation: "IBU Machine Learning Summer School 2024, University of Bern"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output:
  html_document:
    self_contained: true
    toc: true # table of content true
    toc_float: true
    toc_depth: 2  # upto two depths of headings (specified by #, ## and ###)
    theme: spacelab
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

In this exercise, we will performa alinear regression using the cement datset. Thirteen samples of Portland cement were set. For each sample, the percentages of the four main chemical ingredients was accurately measured (X1 - X4). While the cement was setting the amount of heat evolved was also measured (Y).

**Source:** Woods, H., Steinour, H.H. and Starke, H.R. (1932) Effect of composition of Portland cement on heat evolved during hardening. Industrial Engineering and Chemistry, 24, 1207–1214.

We are interested in the following questions/ tasks:

 - Visualize the data, what do you observe? 
 - Perform Linear Regression using all four explanatory variables and describe the results
 - Perform linear regression using only X1 and X2 as explanatory variables
 - Based on the previous results, what do you believe would happen if you use only X3 and X4 as explanatory variables? Would you get siginficant effects or not?
 - What would you get if you use only X1 and X3? 
 - Check your answers by performing the corresponding linear regression models
 
<br>

```{r libraries, echo=FALSE,warning=FALSE,message=FALSE}
# load required libraries
library(dplyr)
library(MASS)
library("monomvn")
#library(VIM)
library(naniar)
library(tidyverse)
library(lmtest)
library(ggplot2)
library(GGally)
library(car)

```


<!-- ## Summary of the Script -->

<!-- - Descriptive Statistics and data visualization -->
<!--    - Assess data structure -->
<!--    - summary stats -->
<!--    - Visualize missingness in data -->
<!--    - visualize correlations -->
<!--    - data distribution -->

<!-- - Modeling: -->
<!--    - Linear Regression: A linear model (lm) to model the continuous response variable (y). -->
<!--    - Assessment of Model summary metrics -->
<!--    - Assessment of assumptions of Linear Regression -->
<!--    - Issue of Multicoliniarity  -->
<!--    - Model selection (backward and forward using stepAIC) -->
<!--    - Model selection (likelihood Ratio Test) -->


## Data Description

Load and print the data set:

```{r Setting up}
data <- cement
# Find the index of the "y" column
outcome_index <- which(colnames(data) == "y")
# Rename the column to "labels" and convert it to a factor
colnames(data)[outcome_index] <- "outcome"
head(data)
```

Here is the structure of the data:

```{r}
str(data)
```

Another way to get an overview of the data is by using the `summary` function:

```{r}
summary(data)
```


###  Visualize the data

Let's check if we have any missing data:

```{r}
#Assess the missing vales in the data set
gg_miss_var(data)
```


<!-- Let's also look ath the Missing Data Plot (I don't get the plot tbh :) -->


<!-- ```{r} -->
<!-- # I cannot install the VIM pagacke -->

<!-- # par(mar = c(10, 2, 2, 2)) -->
<!-- # matrixplot(data, sortby = "outcome", main = "Missing Data Plot", cex.axis = 1) -->
<!-- # mtext("Variable names", side = 1, line = 5) -->

<!-- ``` -->

We should also assess the correlations between numerical predictors:

```{r}
cormat <- cor(data %>% select_if(is.numeric))
cormat
# plot the correlati on matrix
cormat %>% as.data.frame %>% mutate(var2=rownames(.)) %>%
    pivot_longer(!var2, values_to = "value") %>%
    ggplot(aes(x=name,y=var2,fill=abs(value),label=round(value,2))) +
    geom_tile() + geom_label() + xlab("") + ylab("") +
    ggtitle("Correlation matrix of predictors") +
    labs(fill="Correlation\n(absolute):")
```

Or we can also plot the pairwise distributions of the values:

```{r}

pair_plot <- ggpairs(data) +
    theme_minimal()
print(pair_plot)

```


It usually is helpful to visualize further the distribution of the numerical variables:

```{r, message=F}
data %>%
    pivot_longer(everything(), values_to = "value") %>%
    ggplot(aes(x = name, y = value)) +
    geom_boxplot(outlier.shape = NA) +
    geom_jitter(size = 0.7, width = 0.1, alpha = 0.5) +
    theme_minimal() +
    facet_wrap(~name, scales = "free")
    
    
data %>% gather(key, value) %>%
    ggplot(aes(x = value)) + 
    geom_histogram(alpha=0.5, position = 'identity')+
    
    scale_fill_discrete()+
    theme_minimal() +
    facet_wrap(~key, scales="free") +
    labs(title = "Distribution of Numerical variables")+
    theme(plot.title = element_text(hjust = 0.5))

```

## Data Modelling

### Linear Model with all explanatory variables

Let's fit a linear model to the data. In this first iteration we perform the regression using all four explanatory variables.

```{r analysis}
# Fit a linear regression model to the data, using all explanatory variables. 
full_model <- lm(outcome ~ ., data=data)

# Print model summary
summary(full_model)
```

#### Explanation of the linear model

The model summary shows the following:

**Residuals**

This section provides a summary of the residuals (differences between observed and predicted values):

- Min: Minimum residual value.

- 1Q: First quartile (25th percentile) of the residuals.

- Median: Median (50th percentile) of the residuals.

- 3Q: Third quartile (75th percentile) of the residuals.

- Max: Maximum residual value.


**Coefficients**

This section shows the coefficients for each predictor:

- Estimate: The estimated effect of the predictor on the outcome variable.

- Std. Error: The standard error of the estimate.

- t value: The t-statistic for the hypothesis test that the coefficient is different from zero.

- Pr(>|t|): The p-value for the hypothesis test. A smaller p-value indicates stronger evidence against the null hypothesis that the coefficient is zero.


**Residual standard error**

The residual standard error measures the average amount that the observed values deviate from the fitted values. It is calculated using the residuals. The degrees of freedom here is 8.
Multiple R-squared and Adjusted R-squared:

*Multiple R-squared*: Indicates the proportion of variance in the dependent variable that is predictable from the independent variables. In this case, about 98.24% of the variance is explained by the model.

*Adjusted R-squared*: Adjusts the R-squared value for the number of predictors in the model. This value is slightly lower (97.36%), accounting for the number of predictors.

**F-statistic**

The F-statistic tests the overall significance of the model. It compares the model with no predictors (intercept-only model) to the specified model. The p-value here (4.756e-07) is very small, indicating that the model as a whole is statistically significant.

In summary, the model explains a high proportion of the variance in the outcome variable, although not all predictors are statistically significant. The overall model is significant, as indicated by the F-statistic.

Are any of the independent variable significant?

The coefficient for x1 is 1.5511 with a p-value of 0.0708, which is marginally significant.

**Question**: When you compare the outcome to the correlation test do you observe any peculiarities? If so what could be the reason? 

We see that only x1 seems to be somewhat statistically significant. However, variables x2 and x4 correlate stronger with the outcome variable.
This is likely due to multicolienarity which occurs when two or more predictors in a regression model are highly correlated, making it difficult to isolate the individual effect of each predictor on the outcome variable.

In order to corroborate this assumption, we can calculate the variance inflation factor:

```{r}
vif(lm(outcome ~ ., data = data))
```

Usually, variance inflation factors > 10 are considered to indicate severe muticolinearity.

Implications:

- Inflated Standard Errors: The standard errors of the regression coefficients are inflated due to multicollinearity. This leads to wider confidence intervals and less precise estimates, making it difficult to detect significant predictors.

- Unreliable Coefficients: The estimated coefficients may be unreliable. Small changes in the data can lead to large changes in the estimated coefficients, reducing the model's stability.

- Insignificant p-values: Despite high correlations with the outcome, the predictors may have insignificant p-values due to inflated standard errors, as seen with x4 in your model.

<br>

#### Assess Assumptions of linear regression

R has the convenient functionality to produce a series of diagnostic plots by
calling:

```{r assumptions}
par(mfrow = c(2,2))
plot(full_model)
```


**Plot: Residual versus fitted**:

This plot checks the linearity assumption and identifies any obvious patterns.

Interpretation:

- If the points are randomly scattered around the horizontal line at zero, it suggests that the linearity assumption is reasonable.

- If there is a clear pattern (e.g., a curve), it indicates that the relationship between the predictors and the outcome might not be linear.

- Any systematic structure in this plot suggests that there is information left in the residuals that the model hasn't captured.

**Plot: Q-Q Residuals**:

This plot checks for the normality assumption of the pot. 

Interpretation:

- The points should lie approximately along the reference line if the residuals are normally distributed.

- Deviations from the line indicate departures from normality. Outliers will appear as points far from the line, and a systematic departure from the line (e.g., an S-shape) indicates that the residuals are not normally distributed.

**Plot: Scale Location**

Checks the homoscedasticity (constant variance) assumption.

- The plot shows the square root of the standardized residuals against the fitted values.

- The points should be randomly scattered around a horizontal line. If the points spread out as the fitted values increase (funnel shape), it suggests heteroscedasticity (non-constant variance).


**Plot: Residuals versus Leverage**

Purpose: Identifies influential observations that might unduly affect the model.

Interpretation:

- This plot combines standardized residuals with leverage.
  
- Points with high leverage have a large impact on the estimated coefficients.

- Cook’s distance lines (usually shown as dashed lines) help identify influential points. Points outside these lines might be influential and should be investigated further.


Overall, the data set fits the assumptions for a linear model, except that the multicolinearity is possibly an issue.

### Linear Model with less predictors

#### Model with x1, x2

Now, let's simplify the model and only use two variables for x1 and x2 to fit 
the linear model:


```{r}

reduced_model <- lm(outcome ~ x1 + x2, data = data)
summary(reduced_model)
```
In the full model, which included more predictors, none of the variables were statistically significant. However, in this reduced model with only x1 and x2, both variables are highly significant.

This change in significance can be attributed to multicollinearity in the full model. When predictors are highly correlated, their individual contributions to the outcome can be difficult to distinguish, leading to inflated standard errors and non-significant p-values. By removing collinear predictors, the reduced model clarifies the relationships between x1, x2, and the outcome, resulting in significant coefficients.


#### Model with x3, x4 & Model with x1 and x3

Basically, we now want to ask the question:

Which outcomes  do you expect when fitting a linear models:
a) outcome ∼ X3 + X4? 
b) outcome ∼ X2 + X4?
:

The first question to ask is how correlated x3 and x4 are. As we have seen before
in the correlation analysis, these factors are not correlated (R2 of 0.03). Thus, 
we should not run into the issue of multicolinearity.

```{r }
# Fit a linear regression model to the data, using all explanatory variables. 
model_a <- lm(outcome ~ x3 + x4, data=data)

# Print model summary
summary(model_a)
```

Contrarily, when we look at the correlation of x1 and x3, we see that these are
moderately correlated values. However, the model still considers x1 to be significant, but
not x3.

```{r}
# Fit a linear regression model to the data, using all explanatory variables. 
model_b <- lm(outcome ~ x1 + x3, data=data)

# Print model summary
summary(model_b)
```

<!-- ## Model selection -->

<!-- Now we have manually investigated the effects of multicolinearity by selecting -->
<!-- arbitrarily some variables and leaving other variables out. We now want to do  -->
<!-- this systematically by automatically selecting all possible combinatinos and -->
<!-- by then mathematically compare the models. -->

<!-- For this pu -->


<!-- # Model Selection -->
<!-- Start with the full model. -->
<!-- Eliminate variables using backward selection.  -->
<!-- Eliminate variables using forward selection.  -->

<!-- Are the models different? Can you explain what happened? which model is better? -->

<!-- Given the data size is AIC the best approach for model selection? -->


<!-- ```{r model selection} -->


<!-- full_model <- lm(outcome ~ ., data=data) -->
<!-- forward_model <- stepAIC(full_model, direction="forward") -->
<!-- backward_model <- stepAIC(full_model, direction="backward") -->

<!-- summary(forward_model) -->
<!-- summary(backward_model) -->


<!-- # Also perform likelihood ratio test -->
<!-- # Do you understand the output? -->
<!-- # Which model is better according to the LRT test? -->

<!-- lrtest(full_model, backward_model)  -->


<!-- ``` -->


