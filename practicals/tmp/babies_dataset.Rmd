---
title: "Logistic_regression_MLSS"
author: "Aparna Pandey"
date: "2024-05-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Logistic Regression (Babies Dataset)

The data set baby.dat contains data from a study in which clinicians measured 
several clinical variables of premature babies. The response variable, Survival,
indicates whether the babies survived (Survival = 1) or not (Survival = 0).

```{r libraries, echo=FALSE,warning=FALSE,message=FALSE}
# load required libraries
library(tidyverse)
library(naniar)
library(VIM)
library(knitr)
library(kableExtra)
library(ggplot2)
library(GGally)

library(MASS)
library(data.table)
library(lmtest)

library(groupdata2)
```

## Summary of the Script

- Descriptive Statistics and data visualization
   - Assess data structure
   - summary stats
   - Visualize missingness in data
   - visualize correlations
   - data distribution
   - and so on. Some of these are redundant but I have them here for now.

- Modeling:
   - Logistic Regression: A generalized linear model (glm) with a binomial family to model the binary response variable (Survival).
   - Assessment of Model summary metrics
   - Assessment of Model accuracy
   - Cross validation
   - Model selection (backward and forward using stepAIC)
   - Model selection (likelihood Ratio Test)

```{r wd, echo=FALSE,warning=FALSE,message=FALSE}

# set the working directory
setwd("C:/Users/aparn/Desktop/Uni_courses_2022/ML_ss/data")
```


```{r Setting up}

# load data
data <- read.table("data/baby.dat", header=TRUE)

# Find the index of the "Survival" column
label_index <- which(colnames(data) == "Survival")

# Rename the column to "labels" and convert it to a factor
colnames(data)[label_index] <- "labels"
data$labels = as.factor(data$labels)

# Assess the structure of the dataset
str(data)

# convert categorical independent variables into factors
# we have none

# Separate Categorical and continuous independe

numeric_variables <- data %>% 
    select_if(~ !is.factor(.)) %>% 
    colnames()
categorical_variables <- data %>% 
    select_if(~ is.factor(.)) %>% 
    colnames()

```

```{r data exploration and Data Visualization}
# Get summary statistics of the dataset
summary(data)

# Assess the proportion different labels types
proportion <-table(data$labels)

# for visualization purpose only
# Convert the frequency table to a data frame for better formatting options
proportions_df <- data.frame(Outcome = names(proportion), count = as.vector(proportion))

# Use the kable() function to output the table in kable format with a heading
proportions_df %>%
  kable(caption = "Frequency of different survival outcome") %>%
    kable_styling(latex_options = "HOLD_position")

#Assess the missing vales in the data set
gg_miss_var(data)
par(mar = c(10, 2, 2, 2))
matrixplot(data, sortby = "labels", main = "Missing Data Plot", cex.axis = 1)
mtext("Variable names", side = 1, line = 5)


# Assess correlations between  numerical predictors
cormat <- cor(data %>% keep(is.numeric))

# plot the correlation matrix
cormat %>% as.data.frame %>% mutate(var2=rownames(.)) %>%
  pivot_longer(!var2, values_to = "value") %>%
  ggplot(aes(x=name,y=var2,fill=abs(value),label=round(value,2))) +
  geom_tile() + geom_label() + xlab("") + ylab("") +
  ggtitle("Correlation matrix of predictors") +
  labs(fill="Correlation\n(absolute):")

col_indices <- which(!colnames(data) %in% categorical_variables)
pair_plot <- ggpairs(data, columns = col_indices, aes(color = labels)) +
    theme_minimal()
print(pair_plot)

# Visualize distribution of the numerical variables:
data %>% dplyr::select(c(-categorical_variables,"labels")) %>%
    pivot_longer(!labels, values_to = "value") %>%
    ggplot(aes(x=factor(labels), y=value, fill=factor(labels))) +
    geom_boxplot(outlier.shape = NA) + geom_jitter(size=.7, width=.1, alpha=.5) +
    scale_fill_manual(values=c("steelblue", "orangered1")) +
    labs(fill="Survival:") +
    theme_minimal() +
    facet_wrap(~name, scales="free")

data %>% gather(key, value, -labels) %>%
    ggplot(aes(x = value, fill = factor(labels))) + 
    geom_histogram(alpha=0.5, position = 'identity')+
    labs( fill = "Survival")+
    scale_fill_discrete()+
    theme_minimal() +
    facet_wrap(~key, scales="free") +
    labs(title = "Distribution of Numerical variables")+
    theme(plot.title = element_text(hjust = 0.5))


#  Assess if there is obvious association between categorical variables and the
# outcome
if (length(categorical_variables)>1){
data %>% 
  dplyr::select(all_of(categorical_variables)) %>% 
  pivot_longer(!labels, values_to = "value") %>%
  ggplot(aes(x = factor(value), fill = factor(labels))) +
  scale_fill_manual(values = c("steelblue", "orangered1")) +
  geom_bar(position = "fill", alpha = 0.7) +
  theme_minimal() +
  labs(fill = "Survival:") +
  facet_wrap(~name, scales = "free")+
    theme(plot.title = element_text(hjust = 0.5))
}


```


```{r analysis}
# Fit a logistic regression model to the data, using all explanatory variables. 
full_model <- glm(labels ~ ., data=data, family=binomial)
summary(full_model)

#  Do you understand all the metrics in the model summary and their meaning/importance?
#  assess the coefficients of the model variables and associated confidence interval.
#  what do they mean?
exp(cbind(OR = coef(full_model), confint(full_model))) 

# Predict the response variable on the same data and calculate insample misclassification rate
predicted <- ifelse(predict(full_model, data, type="response")  >= 0.5, 1, 0)
misclassification_rate_full <- mean(predicted != data$labels)
misclassification_rate_full


```

```{r model selection}
# Start with the full model.
# Eliminate variables using backward selection. 
# Eliminate variables using forward selection. 

# Are the models different? Can you explain what happened? which model is better?

full_model  <- glm(labels ~ ., data=data, family=binomial)
forward_model <- stepAIC(full_model, direction="forward")
backward_model <- stepAIC(full_model, direction="backward")

summary(forward_model)
summary(backward_model)


# Predict the response variable on the same data and calculate insample misclassification rate
predicted <- ifelse(predict(forward_model, data, type="response")  >= 0.5, 1, 0)
misclassification_rate_forward <- mean(predicted != data$labels)
misclassification_rate_forward

# Predict the response variable on the same data and calculate insample misclassification rate
predicted <- ifelse(predict(backward_model, data, type="response")  >= 0.5, 1, 0)
misclassification_rate_backward <- mean(predicted != data$labels)
misclassification_rate_backward 


# Also perform likelihood ratio test
# Do you understand the output?
# Which model is better according to the LRT test?

lrtest(full_model, backward_model) 


```


```{r cross-validation}
# Estimate the expected misclassification rate (for babies that were not in the study) 
# Use cross validation 
# Is this misclassification different from insample miscalssification? Can you explain why?

# what are the advantages of loocv and k_foldcv?


# Function for leave one out cross-validation
logistic_loocv <- function(formula, data) {
  
  #' Function for eave one out cross-validation
  #'
  #' Arguments for the function:
  #' 
  #' formula = formula for the logistic model e.g.: labels ~ Weight + Age + X1.Apgar + X5.Apgar + pH
  #' 
  #' data =The dataset containing the variables used in the model.

  # calculates the number of rows in data
  n <- nrow(data)
  # create a logical vector of length n to store the misclassification results for each observation.
  misclassification <- logical(n)
  # iterate over each observation in the dataset.
  for (i in 1:n) {
    # fit a logistic regression model using the formula and the dataset with the current observation removed
    model <- glm(formula, data = data[-i, ], family = "binomial")
    # set the cut-off probability
    #  predict the class labels for the current observation using the fitted model.
    predicted <- ifelse(predict(model, data[i,], type="response")  >= 0.5,1,0)
    #compare the predicted class with the true class label (data$labels[i]) for the current observation. 
    misclassification[i] <- predicted != data$labels[i]
  }
  # calculate the mean of the misclassification vector
  return(mean(misclassification))
}


# Function for 5 fold cross-validation
logistic_5fold_cv <- function(formula, data) {
   #' Function for eave one out cross-validation
  #'
  #' Arguments for the function:
  #' 
  #' formula = formula for the logistic model e.g.: labels ~ Weight + Age + X1.Apgar + X5.Apgar + pH
  #' 
  #' data =The dataset containing the variables used in the model.
  
  # calculates the number of rows in data
  n <- nrow(data)
  # create index numbers for the rows
  row_indices = 1:n
  # determine the no. of observations in each fold
  fold_size <- ceiling(n/5)
  # create a logical vector of length n to store the misclassification results for each observation.
  misclassification <- numeric(n)
  # iterate 5 times through the data set (since 5 fold cv)
  for (i in 1:5) {
    # if no. of observations can be less than fold size in the last iteration, 
    # therefore take all the remaining observations 
    if (i == 5) {
      test_indices <- sample(row_indices, length(row_indices), replace = FALSE)
      
    } else {
      # randomly sample observation from the data 
      test_indices <- sample(row_indices, fold_size, replace = FALSE)
      
    }
    # remove the sampled indices form the data
    row_indices = setdiff(row_indices, test_indices)
    
    # use data corresponding to sampled indices as test data and remaining as training data
    train_data <- data[-test_indices, ]
    test_data <- data[test_indices, ]
    # fit a logistic regression model using the formula and the training data 
    model <- glm(formula, data = train_data, family = "binomial")
    # set the cut-off probability
    #  predict the class labels for the current test data using the fitted model.
    predicted <- ifelse(predict(model, newdata = test_data, type = "response") >= 0.5, 1, 0)
    #compare the predicted class with the true class label for the current test data
    misclassification[test_indices] <- predicted != test_data$labels
    
  }
   # calculate the mean of the misclassification vector
  return(mean(misclassification))
}

# determind the misclassification rate following cross validation for all the models
logistic_loocv(formula(full_model), data = data)
logistic_loocv(formula(backward_model), data = data)
logistic_loocv(formula(forward_model), data = data)


```
